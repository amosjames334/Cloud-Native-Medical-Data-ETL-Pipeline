# Medical Data ETL Pipeline

A production-grade, cloud-native ETL pipeline for processing FDA drug approvals and clinical trial data using Apache Airflow, Kubernetes, and AWS S3.

## ğŸ¯ Project Objectives

- Demonstrate data engineering best practices in healthcare domain
- Build scalable, containerized data processing workflows
- Implement proper data quality and governance controls
- Create a portfolio-ready project for data engineering roles

## ğŸš€ Getting Started

For complete setup instructions, including prerequisites, installation, and CI/CD configuration, please refer to [SETUP_INSTRUCTIONS.md](SETUP_INSTRUCTIONS.md).

**Quick Links:**
- [Prerequisites](SETUP_INSTRUCTIONS.md#part-1-prerequisites)
- [Airflow Installation](SETUP_INSTRUCTIONS.md#part-5-airflow-installation-git-sync-method)
- [CI/CD & Dashboard](SETUP_INSTRUCTIONS.md#part-8-cicd-pipeline-and-frontend-dashboard-guide)


## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚
â”‚  - FDA OpenFDA  â”‚
â”‚  - ClinicalTrialsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow DAG    â”‚â—„â”€â”€â”€â”€â”€â”¤  GitHub Actions  â”‚
â”‚  (Orchestrator) â”‚      â”‚  (CI/CD)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kubernetes    â”‚      â”‚  Frontend        â”‚
â”‚   Pod Operator  â”‚      â”‚  Dashboard       â”‚
â”‚  (Transform)    â”‚      â”‚  (Monitoring)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS S3        â”‚
â”‚  (Data Lake)    â”‚
â”‚  Partitioned    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Data Sources

### 1. FDA OpenFDA API
- Drug approvals and adverse events
- Public, no authentication required
- Endpoint: `https://api.fda.gov/drug/event.json`

### 2. ClinicalTrials.gov API
- Clinical trial information
- Public access
- Endpoint: `https://clinicaltrials.gov/api/v2/studies`

## ğŸš€ Features

- **Containerized Processing**: All transformations run in isolated Docker containers
- **Kubernetes Orchestration**: KubernetesPodOperator for scalable execution
- **S3 Data Lake**: Organized with date partitioning (year/month/day)
- **Data Quality Checks**: Built-in validation and error handling
- **Incremental Loading**: Date-based extraction to avoid reprocessing
- **CI/CD Pipeline**: Automated deployment and DAG triggering via GitHub Actions
- **Real-time Monitoring**: Web-based dashboard for DAG runs and task status
- **Email Notifications**: Automatic alerts on deployment and DAG failures
- **API Integration**: RESTful API for programmatic access

## ğŸ› ï¸ Technology Stack

- **Orchestration**: Apache Airflow 2.7+
- **Container Runtime**: Docker, Kubernetes
- **Cloud Storage**: AWS S3
- **Languages**: Python 3.9+
- **Key Libraries**: pandas, requests, boto3, great_expectations

## ğŸ“ Project Structure

```
medical-etl-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ SETUP_INSTRUCTIONS.md
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.transform
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ medical_etl_dag.py
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ pipeline_config.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fda_extractor.py
â”‚   â”‚   â””â”€â”€ clinicaltrials_extractor.py
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ drug_transformer.py
â”‚   â”‚   â””â”€â”€ data_quality.py
â”‚   â”œâ”€â”€ loaders/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ s3_loader.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â”œâ”€â”€ test_transformers.py
â”‚   â””â”€â”€ test_loaders.py
â”œâ”€â”€ kubernetes/
â”‚   â””â”€â”€ pod-template.yaml
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create-airflow-user.ps1
â”‚   â””â”€â”€ deploy-frontend.ps1
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ styles.css
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ ci-cd.yml

```

## ğŸ“ Learning Outcomes

This project demonstrates:
- ETL pipeline design and implementation
- Cloud-native architecture patterns
- Container orchestration with Kubernetes
- Healthcare data compliance considerations
- Data quality management
- Infrastructure as Code principles


## ğŸ” Data Governance

- All data sources are publicly available
- No PHI (Protected Health Information) processed
- HIPAA considerations documented
- Data retention policies implemented

## ğŸ“ Features Implemented

âœ… **Core ETL Pipeline**
- Data extraction from FDA and ClinicalTrials.gov APIs
- Kubernetes-based transformation processing
- S3 data lake with partitioning
- Data quality validation

âœ… **CI/CD Pipeline**
- Automated DAG validation and testing
- Automatic deployment on code changes
- DAG triggering via GitHub Actions
- Email notifications

âœ… **Monitoring Dashboard**
- Real-time DAG run status
- Task execution visualization
- Error log viewing
- Manual DAG triggering
- Performance statistics


## ğŸ¤ Contributing

This is a portfolio project, but suggestions welcome via issues. Learn, grow, and have fun!

## ğŸ“„ License

MIT License - Free for educational and portfolio use

## ğŸ“§ Contact

[Amos Jaimes]  
[jaimes.a@northeastern.edu]  
[LinkedIn Profile](https://www.linkedin.com/in/amos-jaimes-a8107621b/)  
[GitHub Profile](https://github.com/amosjames334)

---

**Note**: This project uses public APIs and sample data. For production use with real patient data, additional HIPAA compliance measures would be required.